// src/app/api/ai/summary/route.ts
import { NextRequest, NextResponse } from "next/server";
import { z } from "zod";
import OpenAI from "openai";
import { cookies as nextCookies } from "next/headers";
import { createServerClient } from "@supabase/ssr"; // back to SSR version


export const runtime = "nodejs";
export const dynamic = "force-dynamic";

const bodySchema = z.object({
  transcript: z.string().min(10, "Transcript must be at least 10 characters."),
});

const MOCK = process.env.NEXT_PUBLIC_MOCK_AI === "1";

export async function POST(req: NextRequest) {
  try {
    // --- Auth (feature-flag to unblock Next16 cookie issue) ---
    const REQUIRE_AUTH = process.env.NEXT_PUBLIC_REQUIRE_AUTH !== "0";

    if (REQUIRE_AUTH) {
      // NOTE: We'll re-enable real auth once the cookie adapter is patched.
      // For now, respond with a clear message if someone turns this back on without fixing cookies.
      return NextResponse.json(
        { error: "Auth temporarily disabled via NEXT_PUBLIC_REQUIRE_AUTH=0 (Next16 cookie adapter issue)" },
        { status: 503 }
      );
    }
    // proceed unauthenticated for local testing



    // --- Parse & validate ---
    const json = await req.json();
    const { transcript } = bodySchema.parse(json);

    // --- Mock mode (optional for fast UI wiring) ---
    if (MOCK) {
      return NextResponse.json({
        good: "Clear message of support and gratitude.",
        bad: "Some rambling; a few repeated points.",
        ugly: "Background noise reduces clarity in places.",
        lesson: "Keep it concise; capture one story per entry.",
      });
    }

    // --- OpenAI call ---
    if (!process.env.OPENAI_API_KEY) {
      return NextResponse.json(
        { error: "OPENAI_API_KEY not configured" },
        { status: 500 }
      );
    }

    const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

    const system = [
      "You summarize short parent audio transcripts as four bullets:",
      "Good (what worked), Bad (what could be improved),",
      "Ugly (harsh/frictional moments), Lesson (actionable takeaway).",
      "Return strict JSON with keys: good, bad, ugly, lesson. Keep each to 1â€“2 sentences.",
    ].join(" ");

    const userPrompt = `Transcript:\n${transcript}\n\nReturn JSON only.`;

    const start = Date.now();

    const completion = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      temperature: 0.2,
      response_format: { type: "json_object" },
      messages: [
        { role: "system", content: system },
        { role: "user", content: userPrompt },
      ],
      max_tokens: 400,
    });

    const durationMs = Date.now() - start;
    const content = completion.choices[0]?.message?.content ?? "{}";

    let parsed: any;
    try {
      parsed = JSON.parse(content);
    } catch {
      // Guard in case of malformed output
      parsed = {
        good: "Clear positive point identified.",
        bad: "A specific area to improve was mentioned.",
        ugly: "One friction point was surfaced.",
        lesson: "There is a concise, actionable takeaway.",
      };
    }

    // Optional: later you can log latency in an events table
    // await supabase.from("events").insert({
    //   user_id: user.id,
    //   type: "ai_summary",
    //   meta: { duration_ms: durationMs },
    // });

    return NextResponse.json(parsed, {
      headers: { "x-ai-latency-ms": String(durationMs) },
    });
  } catch (err: any) {
    const msg =
      err?.issues?.[0]?.message /* zod */ ||
      err?.message ||
      "Unexpected error";
    return NextResponse.json({ error: msg }, { status: 400 });
  }
}
